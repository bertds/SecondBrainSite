<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">


</head>

<body>

  <div class="grid">
    <nav>
      <ul>
        <br>
        <li><a href="https://bertds.github.io/SecondBrainSite/">Second Brain</a></li>
        <br>
      </ul>
    </nav>
    <article>
      <div id="content"><h1 id="neurale-netwerken">Neurale Netwerken</h1>

<p>Tags: <span class="tag">AI</span> <span class="tag">andere</span><br />
Date: 2021-02-28<br />
Type: <a href="cursus-topic.html">Cursus topic</a><br />
Related: <!-- Links to pages not referenced in the content --><br />
Source : </p>

<h2 id="notities">Notities</h2>

<h3 id="biologisch-neuron">Biologisch neuron</h3>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_11.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_11.jpg" /><br />
Artificeel NN is geinsipireerd op onze hersenen. </p>

<h3 id="artificieel-neuron">Artificieel Neuron</h3>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_12.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_12.jpg" /><br />
Belangrijkheid van input data wordt bepaald obv weights.<br />
Treshold bepaald of resultaat activatie functie naar volgende laag gaat/afgevuurd wordt. Alle neuronen van eenzelde laag hebben zelfde activatiefunctie. Uitzondering de outputlaag. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_13.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_13.jpg" /></p>

<h3 id="logistic-regression">Logistic Regression</h3>

<p>Activatiefunctie = Sigmoid == Logistic Regression</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_14.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_14.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_15.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_15.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_16.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_16.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_17.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_17.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_18.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_18.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_19.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_19.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_20.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_20.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_21.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_21.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_22.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_22.jpg" /><br />
Kracht van NN is dat het laag na laag features gaat bij creeÃ«ren. De output van de ene neuron is een nieuwe feature die input is voor de volgende neuron. Hogere orde features worden door NN zelf aangemaakt. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_23.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_23.jpg" /><br />
Meer neuronen, meer lagen &gt; complexere scheidingslijnen.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_24.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_24.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_25.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_25.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_26.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_26.jpg" /></p>

<h3 id="netwerk-architectuur">Netwerk architectuur</h3>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_27.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_27.jpg" /></p>

<h3 id="feedforward-nn">Feedforward NN</h3>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_28.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_28.jpg" /><br />
Klassieke NN zijn feedforward. Informatie stroomt van input naar output in 1 richting. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_29.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_29.jpg" /></p>

<h4 id="mnist">MNIST</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_30.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_30.jpg" /><br />
input layer met 784 (28x28 pixels) neuronen<br />
nomalisatie door pixel waarde te delen door 255.0<br />
deze is nodig voor de activatie functie (waarde tss -1 en +1)<br />
output layer : 10 verschillende klasses met zekerheidspercentage per klasse</p>

<p>Hoe veel hiddenlayers en neuronen moet je gebruiken ?<br />
Hoe meer neuronen, hoe meer weights hoe meer rekenkracht nodig.<br />
Vuistregel 100 per hidden layer of evenveel als input om mee te beginnen en nadien bijstellen. Beter meer layers toevoegen ipv meer neuronen per laag te nemen. </p>

<p>Als je meer dan 2 klasses hebt moet je meerdere output neuronen hebben.<br />
Als je maar 2 klasses hebt dan kan je met 1 output neuron toekomen.</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_31.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_31.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_32.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_32.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_33.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_33.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_34.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_34.jpg" /></p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_35.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_35.jpg" /></p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_36.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_36.jpg" /><br />
Softmax weegt alle uitgangen af tov van elkaar als de uitgang van 1 neuron wordt bepaald. Nut ? Hierdoor gaan alle neuronen zeggen hoe waarschijnlijk dat dit de juiste klasse is tov de andere klasses.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_37.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_37.jpg" /></p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_38.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_38.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_39.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_39.jpg" /></p>

<h4 id="backpropagation">Backpropagation</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_40.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_40.jpg" /><br />
Goede learning rate : niet te snel en niet te traag<br />
Tegenwoordig nog weinig van aantrekken omdat optimizer dit regelt. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_41.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_41.jpg" /></p>

<h3 id="activatie-functie">Activatie functie</h3>

<h4 id="step-function">step function</h4>

<p>back propagation gaat niet omdat je de afgeleide niet kan berekenen. Dus NIET gebruiken.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_42.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_42.jpg" /></p>

<h4 id="linear-function">Linear functioN</h4>

<p>Niet als hidden layer omdat je geen lineair verband kan vinden enkel lineaire scheidingslijnen. Maar wel te gebruiken bij regresie als je bijv de effectieve koers van een aandeel wil voorspellen.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_43.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_43.jpg" /></p>

<h4 id="sigmoid">sigmoid</h4>

<p>Enkel actief in het actief gebied. Probleem van Vanishing gradient.<br />
Wel te gebruken bij binaire classificatie bijv output layer. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_44.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_44.jpg" /></p>

<h4 id="tanh">tanh</h4>

<p>Zelfde probleem Vanishing Gradient<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_45.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_45.jpg" /></p>

<h4 id="relu">relu</h4>

<p>Je kan er complexe scheidingslijn mee vormen omdat het lineair is. Negatieve sommatie zijn 0 en positieve waarde = som.<br />
Snel te berekenen.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_46.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_46.jpg" /></p>

<h4 id="leaky-relu">leaky relu</h4>

<p>Rekenintensiever dan Relu.<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_47.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_47.jpg" /></p>

<h4 id="conclusion">conclusion</h4>

<h5 id="hidden-layer">hidden layer</h5>

<ul>
<li>First : Relu</li>
<li>Try : Leaky Relu</li>
<li><p>Usually No Signloid, Tranh, Soft Max</p>

<h5 id="output-layer">Output layer</h5></li>
<li><p>Lineair for regression</p></li>
<li>SoftMax/Sigmoid for classification<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_48.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_48.jpg" /><br />
<h3 id="underfittingoverfitting">Underfitting/Overfitting</h3></li>
</ul>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_49.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_49.jpg" /></p>

<h4 id="l2-regularization">L2 regularization</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_50.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_50.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_51.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_51.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_52.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_52.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_53.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_53.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_54.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_54.jpg" /></p>

<h4 id="dropout">dropout</h4>

<p>Tijdens training !<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_55.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_55.jpg" /><br />
Bij het bepalen van de accuracy kan het zijn dat die op de trainingset slechter is dan op de testset. Dit komt omdat op de trainingset niet alle neuronen actief zijn door de dropout en deze zijn wel actief bij de testset. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_56.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_56.jpg" /><br />
Bewust tijdens training ruis toevoegen. De waarden veranderen hierdoor lichtjes en ziet het model steeds iets andere data en gaat die minder van buiten leren. </p>

<h3 id="gradient-descent-modes">Gradient descent modes</h3>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_57.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_57.jpg" /><br />
Batch size is belangrijke parameter : aantal voorbeelden dat je gaat tonen voor je de weights gaat aanpassen. Extremen : na elk voorbeeld / na volledige epoch.<br />
Epochs = Epics : aantal keer dat je de volledige trainingset toont aan het model<br />
Iterations : aantal keer dat je de weights gaat bijstellen<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_58.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_58.jpg" /></p>

<h4 id="full-gradient-descent">Full gradient descent</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_59.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_59.jpg" /><br />
Elke epoch weights bijstellen. Je hebt weinig rekenkracht nodig, maar meer geheugen omdat je alles bijstellingen voor alle weights per sample moet bijhouden tot als epoch gedaan is. </p>

<h4 id="stochastic-gradient-descent">stochastic gradient descent</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_60.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_60.jpg" /><br />
Elke trainingsample aanpassen. Hierdoor heb je ook minder geheugen nodig, maar meer rekenkracht omdat je bij elke sample een herberekening van de weights moet doen. </p>

<h4 id="batch-gradient-descent">batch gradient descent</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_61.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_61.jpg" /><br />
Tussenvorm. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_62.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_62.jpg" /><br />
Elke oplossing komt uiteindelijk min of meer tot hetzelfde besluit.</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_63.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_63.jpg" /><br />
snel kunnen rekenen vs geheugen limieten</p>

<h4 id="momentum-adaptive-learning-rates">momentum &amp; adaptive learning rates</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_64.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_64.jpg" /><br />
zonder optimizer duurt langer maar kan soms beter oplossing vinden dan met optimizer. </p>

<ul>
<li>momentum : </li>
<li>adaptive learning rates : learning rate constant aanpassen, per weight aparte learning rate</li>
</ul>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_65.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_65.jpg" /><br />
je gaat weight tegengesteld aan fout met een klein beetje bijstellen (in lijn met de learning rate)<br />
van welke technieken maakt optimizer gebruik om ervoor die traning toch een stukje sneller gebeuren (adaptive learning rates en momentum conceptueel uitleggen)<br />
momentum verandert constant<br />
<span class="tag">todo</span> momentum opzoeken<br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_66.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_66.jpg" /></p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_67.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_67.jpg" /></p>

<h5 id="variabel-learning-rate">variabel learning rate</h5>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_68.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_68.jpg" /></p>

<h5 id="adaptive-learning-rate">adaptive learning rate</h5>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_69.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_69.jpg" /><br />
per weight optimale learning rate &gt; extra geheugen nodig om al die learning rates bij te houden</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_70.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_70.jpg" /><br />
niet enkel de weights veranderen in welke richting maar ook de grote waarin ze veranderen veranderd</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_71.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_71.jpg" /><br />
beide combineren</p>

<h4 id="weight-initialization">weight initialization</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_72.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_72.jpg" /><br />
weights moeten random ingesteld worden zodat je niet bij elke neuron in de hidden layers dezelfde situatie hebt en dus eigenlijk met 1 neuron toekomt. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_73.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_73.jpg" /><br />
veel manieren om random in te stellen omdat er met sigmoids werd gewerkt en je in het optimale stuk van de sigmoid terecht kwam.<br />
Nu met Reul is dat niet meer nodig aangezien die altijd actief is als de waarde &gt; 0. </p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_74.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_74.jpg" /><br />
<img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_75.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_75.jpg" /></p>

<h4 id="batch-normalization">Batch normalization</h4>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_76.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_76.jpg" /><br />
Door te normaliseren gaan al je features met dezelfde learning rate even snel veranderen en naar optimaal optimum komen. Anders gaat ene feature sneller naar optimum komen dan ander feature omdat de schaal anders is.<br />
Batch normalization gaat na elke layer normaliseren, dus op de output, om steeds in actiever gebieden van activatiefunctie te zitten voor de volgende layer.<br />
Hierdoor ook minder gevoelig voor overfitting omdat je wat ruis toevoegd door die batch normalization. Daarom dat dropout soms niet wordt toegepast omdat batch normalisation reeds de nodige ruis toevoegd.<br />
https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/</p>

<p><img src="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_77.jpg" alt="Session_01_Introduction_to_Neural_Networks_Annotations_20201024_77.jpg" /><br />
Door batch normalization gaat loss curve sneller naar beneden (rode lijn)</p>
</div>

      <div class="backlinks">

<ul>
<li><a href="ai-home.html">AI@home</a></li>
<li><a href="introductie-deep-learning.html">Introductie Deep Learning</a></li>
</ul>

</div>


    </article>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
   mermaid.initialize();
  </script>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>

  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>

</body>

</html>
