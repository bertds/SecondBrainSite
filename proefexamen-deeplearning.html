<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">


</head>

<body>

  <div class="grid">
    <nav>
      <ul>
        <br>
        <li><a href="https://bertds.github.io/SecondBrainSite/">Second Brain</a></li>
        <br>
      </ul>
    </nav>
    <article>
      <div id="content"><h1 id="proefexamen-vragen-deep-learning">Proefexamen vragen Deep learning</h1>

<p>Q: Welke bewering(en) is/zijn verkeerd met betrekking tot dropout?</p>

<ol>
<li>Dropout is een regularisatietechniek</li>
<li>Dropout vermindert de kans op overfitting</li>
<li>Dropout lost het vanishing gradient probleem op<br />
A: 3<br />
<!--ID: 1621865794374--></li>
</ol>

<p>Q: Welke bewering(en) is/zijn verkeerd ivm weights en biases van een neuraal netwerk?</p>

<ol>
<li>Biases mogen op nul geïnitialiseerd worden</li>
<li>De weights worden op nul geïnitialiseerd</li>
<li>De weights en biases zijn hyperparameters<br />
A:  2 en 3<br />
<!--ID: 1621865794425--></li>
</ol>

<p>Q: Welke van onderstaande neurale netwerk architecturen beschikken over een intern geheugen?</p>

<ul>
<li>Variational autoencoders</li>
<li>GAN</li>
<li>Autoencoders</li>
<li>Convolutional Neural Networks</li>
<li>Recurrent Neural Networks<br />
A: RNN<br />
<!--ID: 1621865794461--></li>
</ul>

<p>Q: Welke bewering is over sigmoid en tanh is/zijn verkeerd?</p>

<ol>
<li>Ze hebben geen last van het vanishing gradient probleem.</li>
<li>In tegenstelling tot bij ReLu moet bij hen een exponentiële functie uitgerekend worden</li>
<li>Het zijn niet-lineaire functies<br />
A: enkel 1<br />
<!--ID: 1621865794488--></li>
</ol>

<p>Q: Welke van de volgende bewering(en) over Epochs in een neuraal netwerk is/zijn correct?</p>

<ol>
<li>Het aantal epochs is het aantal keer dat de volledige training set tijdens training aan het netwerk wordt getoond.</li>
<li>Het aantal epochs is een hyperparameter.</li>
<li>De tijd om een volledige epoch te doorlopen hangt af van de batch size.<br />
A: 1 en 3<br />
<!--ID: 1621865794522--></li>
</ol>

<p>Q: Bij welk type neuraal netwerk wordt KL-divergence toegepast?</p>

<ul>
<li>Variational autoencoder </li>
<li>CNN </li>
<li>LSTM </li>
<li>GRU </li>
<li>GAN<br />
A: VAE<br />
<!--ID: 1621865794558--></li>
</ul>

<p>Q: Welke van de volgende types van Recurrent Neural Netwoks zijn geschikt voor image captioning (een onderschrift bij een afbeelding plaatsen)?<br />
One to Many<br />
Many to One<br />
Many to Many<br />
One to one<br />
A: one to many<br />
<!--ID: 1621865794588--></p>

<p>Q: Welk neuraal netwerk is het meest geschikt voor objectherkenning in afbeeldingen?</p>

<ul>
<li>LSTM</li>
<li>Autoencoder</li>
<li>GAN</li>
<li>Variational autoencoder</li>
<li>Vanilla feedforward neural network</li>
<li>Convolutional neural network<br />
A: CNN<br />
<!--ID: 1621865794620--></li>
</ul>

<p>Q: Leg uit waarom een convolutional neural network typisch minder weights (parameters) heeft dan een fully connected neural network om dezelfde image classification taak uit te voeren.<br />
A: Een CNN heeft minder weights omdat die niet full connected is. Enkel de neurons nodig voor kernel worden gebuikt tijdens training obv een trainingsample. De weights zijn wel shared zodat die bij elk gebruik van een bepaald neuron upgedate worden tijdens de training met de verschillende trainingsamples.<br />
<!--ID: 1622754710699--></p>

<p>Q: Bespreek de verschillende stappen die je doorloopt bij het toepassen van transfer learning bij een image classification problem.<br />
A: De stappen zijn :</p>

<ul>
<li>bereid je data voor zoals je anders doet (analyse, one-hot encoding labels, resolutie images aanpassen, splits de data in training, validatie en testset, gebruik image augmentation)</li>
<li>download het model en weights</li>
<li>bevries alle weights van het model</li>
<li>voeg classificatie lagen toe aan het model die de laatste lagen van het model vevangen (weights staan standaard trainable)</li>
<li>train het model obv eigen data</li>
<li>hyperparameter tuning<br />
<!--ID: 1622754710778--></li>
</ul>

<p>Q: Hoe komt een Skip-Gram word embedding tot stand?<br />
A: Maak voor elke woord uit het window (= x aantal woorden rond het centraal woord ) een one-hot vector. Maak voor het centraal woord ook een one-hot vector. Gebruik de one-hot vectors van de window woorden als output voor een Vanilla NN en de one-hot vector van het kernwoord als input. Train het model zodat het obv de input vector van het kernwoord de output vecvtor kan voorspellen van elk windowwoord. De weights van de input naar de hidden layer is de word embedding voor dat kernwoord.<br />
<!--ID: 1622799262349--></p>

<p>Q: Wat is een Leaky Relu? Vergelijk deze met de gewone Relu en bespreek de voor-en nadelen.<br />
A: De gewone Relu is rekenefficient, de Leaky relu iets minder. Beide zijn niet-lineaire functies en door die achter elkaar te zetten kan je hiermee andere functies nabootsen. Het nadeel van de gewone Relu is, is dat het neuron inactief kan komen (= 0). Dit probleem heeft de Leaky Relu niet.<br />
<!--ID: 1622799262368--></p>

<p>Q: Stel dat je merkt dat de training error grillig op en neer gaat? Wat zou je kunnen doen om dit te verhelpen?<br />
A: Je kan de learning rate aanpassen door een optimizer te gebruiken zodat de learning rate kleinere stappen neemt, je zou een learning rate scheduler callback kunnen toevoegen waarmee je de learning rate voor elke epoch kan sturen (en kleiner maken bij elke epch bijv),  je kan je dropout verlagen zodat er meer neuronen actief blijven tijdens de training.<br />
<!--ID: 1622799262376--></p>

<p>Q: Wat is het voordeel van het gebruik van word embeddings ten opzicht van one-hot encoded representaties van woorden? Bespreek.<br />
A: One-hot encoded neemt veel ruimte in omdat elk woord als one-hot encoded vector moet voorgesteld worden en als je een grote vocabulair hebt dan heb je een matrix waar 90% nullen inzitten. Bijkomend kan hezt model hiermee ook geen relaties tss de woorden die in de zinnen voorkomen gebruiken want elke one-hot vector/woord staat los van elkaar. Dit in tegenstelling tot word embeddings waar de voorstelling per woord compacter is omdat de elk woord een vector heeft die max 300 groot is. Hierbij komt nog dat deze vector de relatie tss woorden weergeeft. De nabijheid van de woorden door die vectoren kan dus door het model gebruikt worden om betere predicties te maken.<br />
<!--ID: 1622799346326--></p>

<p>Q: Wat is een Markov Decision Process en wat is het belang ervan binnen de context van reinforcement learning?<br />
A: nog in te vullen<br />
<!--ID: 1622754710786--></p>
</div>

      <div class="backlinks">

<ul>
<li><a href="ai-home.html">AI@home</a></li>
</ul>

</div>


    </article>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
   mermaid.initialize();
  </script>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>

  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>

</body>

</html>
