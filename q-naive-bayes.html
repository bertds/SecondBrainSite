<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">


</head>

<body>

  <div class="grid">
    <nav>
      <ul>
        <br>
        <li><a href="https://bertds.github.io/SecondBrainSite/">Second Brain</a></li>
        <br>
      </ul>
    </nav>
    <article>
      <div id="content"><p><a href="naive-bayes.html">Naive Bayes</a><br />
<a href="anki.html">anki</a></p>

<p>Q: Hoe verschilt Naive Bayes van andere modellen en waarvoor wordt het vaak gebruikt ?<br />
A: <a href="naive-bayes#voordelen-en-gebruik.html">Voordelen en gebruik</a><br />
Supersnel<br />
iets minder data<br />
veel features<br />
<!--ID: 1609005699302--></p>

<p>Q: Wat is het verschil tss Discriminative en Generative model ?<br />
A: Bij discriminative model leert de classifier de grenzen kennen tss twee klassen. Die bepaalt wat de kans is dat iets tot een bepaalde klasse behoort ? Terwijl een generative model leert de distributie van de verschillende klassen. Wat maakt dat die features tot die bepaalde klasse horen ? Wat is de kans dat een bepaalde klasse bepaalde features heeft ?<br />
<!--ID: 1609005699311--></p>

<p>Q: Wat is een bag of words ?<br />
A: <a href="naive-bayes#bag-of-words.html">Bag of words</a><br />
Bag of words : collectie van unieke woorden die voorkomen in de volledige trainingset (na opkuis, ...). De uiteindelijke feature vector heeft dezelfde dimensie als de bag of words, die kan je gebruiken om classifier te trainen.<br />
<!--ID: 1609005699315--></p>

<p>Q: Welke varianten zijn er op bag of words en hoe verschillen ze ?<br />
A: <strong>Multi-variate Bernoulli Naive Bayes</strong><br />
Er wordt enkel bijgehouden of een woord in de bag of words al dan niet voorkomt in een document (0 of 1) en dus NIET de frequentie van voorkomen. In de praktijk werkt het niet zo goed.<br />
<strong>Multinomial Naive Bayes</strong> :<br />
De frequentie (aantal keer) waarmee een woord uit de bag of words voorkomt in het document wordt bijgehouden. Scikit-learn : <span class="tag">CountVectorizer</span><br />
fit = lijst opbouwen van woorden &gt; feature vector maken (1000den features)<br />
transform = per bericht count van woorden in bericht en mappen met features van fit<br />
<strong>tf-idf transformer</strong><br />
aantal keren dat woord voorkomt in document delen door het aantal keer dat woord voorkomt in volledige trainingset<br />
dit is om belang van het woord te bepalen. hoe uniek is het ? als het veel voorkomt brengt het weinig meerwaarde aan bericht om onderscheid te maken voor klasse bepaling<br />
eerst counts doen<br />
fit op bag of words<br />
<a href="naive-bayes#bag-of-words-varianten.html">Bag of words varianten</a><br />
<!--ID: 1609005699320--></p>

<p>Q: Waar staat tf-idf voor ?<br />
A: tf-idf means term-frequency times inverse document-frequency<br />
<!--ID: 1611305648713--></p>

<p>Q: Wat is de Bayes regel ? Geef de formule en verklaar de delen.<br />
A: $P(H|e) = \dfrac{P(e|H) P(H)}{P(e)}$<br />
<em>Likelihood</em>: $P(e|H)$ How probable is the evidence given that our hypothesis is true ? (hypothese = de stelling die je maakt, evidence = features)<br />
<em>Prior</em> : $P(H)$ How probable was our hypothesis before observing the evidence<br />
<em>Posterior</em>: $P(H|e)$ How probable is our hypothesis given the observed evidence ? (Not directly computable)<br />
<em>Marginal</em>: $P(e)$ How probable is the new evidence under all possible hypotheses ?<br />
<!--ID: 1610879528995--></p>

<p>Q: kans dat iemand kanker heeft bedraagt 1% =&gt; P(kanker) = 0.01<br />
van een test is geweten :</p>

<ul>
<li>in 90% van de gevallen is de test positief als je effectief kanker hebt (sensitiviteit)</li>
<li>in 90% van de gevallen is de test negatief als je geen kanker hebt (specifiteit)<br />
VRAAG :<br />
Als de test positief blijkt, wat is de kans dat je kanker hebt ?<br />
A: $$P(kanker|positief) = \dfrac{P(positief|kanker) * P(kanker)}  {P(positief)}$$<br />
$$P(kanker|positief) = \dfrac{0.9 * 0.01}{0.01 * 0.90 + 0.1 * 0.99} = 0.0833$$<br />
<em>Prior</em> : $P(kanker)$: zonder de testresultaten te kennen, hoe waarschijnlijk is het dat iemand kanker heeft ?<br />
<em>Likelihood</em>: $P(positief|kanker)$ hoe waarschijnlijk is het dat de test positief is wanneer de persoon effectief kanker heeft ?<br />
<em>Marginal</em>: $P(positief)$ Hoe waarschijnlijk is het dat de test positief is ?<br />
<em>Posterior</em>: $P(kanker|positief)$: Wat is de kans dat iemand effectief kanker heeft als de test positief blijkt ?<br />
<!--ID: 1610879529286--></li>
</ul>

<p>Q: Wat is laplacian smoothing ? Waar gebruik je het ?<br />
A: Laplacian smoothing (= add-one smoothing) : kent niet geziene woorden toch een zeker kans van voorkomen toe. Anders gaan je kans berekening bijv bij spam/ham beide op 0 komen door het onbekende woord.<br />
$P(Ws) = \dfrac{C(Ws) + \alpha}{N + \alpha V}$<br />
<!--ID: 1610880918169--></p>

<p>Q: Wat is log likelihood ? Waar gebruik je het ?<br />
A: Het vermenigvuldigen van veel kansen kan leiden tot floaing-point underlow. Dat kan je vermijden door het logaritme te bereken van het product, wat je kan vertalen in de som van de logaritmes :<br />
$log(xy) = log(x) + log(y)$<br />
log likelihood van Ham  en van Spam berekenen en grootste getal heeft meeste waarschijnlijkheid (dit is <em>geen</em> percentage)<br />
<!--ID: 1610882854398--></p>
</div>

      <div class="backlinks">

<ul>
<li><a href="naive-bayes.html">Naive Bayes</a></li>
</ul>

</div>


    </article>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
   mermaid.initialize();
  </script>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>

  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>

</body>

</html>
