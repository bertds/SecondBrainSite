<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">


</head>

<body>

  <div class="grid">
    <nav>
      <ul>
        <br>
        <li><a href="https://github.com/bertds">GitHub</a></li>
        <br>
      </ul>
    </nav>
    <article>
      <div id="content"><p>Q: Wat is een neuraal netwerk ?<br />
A: A neural network (NN) is a massively parallel distributed processor that has a natural propensity for storing experimental knowledge and making it available for use. It resembles the brain in two aspects:</p>

<ul>
<li>Knowledge is acquired by the network through a learning process.</li>
<li>Interneuron connection strenghts known as synaptic weights are used to store the knowledge.<br />
<!--ID: 1610223327141--></li>
</ul>

<p>Q: Wat is een perceptron ?<br />
A: Neuron van een artifieel neuraal netwerk<br />
<!--ID: 1610223327833--></p>

<p>Q: Wat doet de activatie functie in een perceptron ?<br />
A: De activatie functie bepaalt de output van de perceptron. Deze kan binair zijn, een getal/percentage bij regressie of een lijst van kansen op bepaalde labels/klassen.<br />
<!--ID: 1610223327847--></p>

<p>Q: Geef een voorbeeld van een architectuur voor een artificieel netwerk<br />
A: Feedforward Neuraal netwerk, LSTM (long short-term memory)<br />
<!--ID: 1610223327942--></p>

<p>Q: Hoe is een feedforward neuraal netwerk opgebouwd ?<br />
A: <a href="sessie-07-introductie-neurale-netwerken-annotaties-page-020dotjpg.html">Sessie_07_Introductie_Neurale_Netwerken_Annotaties-page-020.jpg</a><br />
<!--ID: 1610223327951--></p>

<p>Q: Wat is backpropagation learning bij neurale netwerken ?<br />
A: een NN leert van achter naar voor, van output naar input. Obv de output van de neuronen wordt bepaald van welke neuronen het gewicht moeten bijgesteld om de beoogde output te hebben en dit wordt gedaan voor alle lagen van de NN.<br />
<!--ID: 1610223327960--></p>

<p>Q: Hoe worden de gewichten bepaalt bij een NN ?<br />
A: Initieel worden de gewichten random ingevuld en is de error functie zeer groot. Obv training (gradien descent met learning rate) worden de gewichten bijgesteld.<br />
<!--ID: 1610223327969--></p>

<p>Q: Wat is er mis bij een NN als de error functie niet zakt ?<br />
A: je NN leert niet bij waarschijnlijk te wijten aan een te grote learning rate.<br />
<!--ID: 1610223327978--></p>

<p>Q: Waarom is de step functie niet goed als activatie functie bij een NN ?<br />
A: Omdat die functie enkel 0 of 1 als output heeft waardoor de gradient descent niet weet welke kant die op moet en welke neuron verantwoordelijk is voor de error in de predictie.<br />
<!--ID: 1610223327986--></p>

<p>Q: Waarom is de lineair function (Adaline) niet goed bij NN als activatiefunctie in hidden layer ? Waarom wel in de input layer ? In welke gevallen wel in de output layer ?<br />
A: Linaire functie blijft lineair en kan hierdoor geen informatie halen uit complexe data.  In de input layer kan ze wel gebruikt worden omdat die de data onveranderd het netwerk instuurt.<br />
Voor de output layer is ze goed bij regressie omdat deze functie het resultaat niet beperkt tot een waarde tss 0 en 1.<br />
<!--ID: 1610223327997--></p>

<p>Q: Waarom is de sigmoid functie niet goed bij NN als activatiefunctie in hidden layer ?<br />
A: Omdat de berekening zwaarder is en door het vanishing gradient probleem waarbij de uitlopers van de functie praktisch niet meer veranderen en de gradient descent dus niet kan bepalen in welke richting die moet evolueren.<br />
<!--ID: 1610223328007--></p>

<p>Q: Wat is het voordeel om de Relu functie te gebruiken als activatiefunctie in een NN ?<br />
A: Relu is niet lineair, en elke functie kan benaderd worden door een combinatie van Relu functies. Zeer efficient in rekenkracht. Een nadeel is wel dat neuronen dood kunnen zijn (sparse activation - veel activaties worden 0) en dode neuronen kunnen niet meer geactiveerd worden.<br />
<!--ID: 1610223328024--></p>

<p>Q: Wat is het voordeel van Leaky Relu tov Relu als activatiefunctie bij NN ?<br />
A: De Leaky Relu gaat niet dood.<br />
<!--ID: 1610223328042--></p>

<p>Q: Welke activatie functie gebruik je het beste voor hidden layer en welke zeker niet ?<br />
A: Voor de hidden layer probeer je eerst ReLu, dan Leaky ReLu. Sigmoid en Tanh niet voor hidden layer.<br />
<!--ID: 1610223328050--></p>

<p>Q: Voor output layer bij een NN welke activatie functie moet je gebruiken voor regressie, classificatie ?<br />
A: Voor de output layer lineair bij regressie. Softmax (meerdere klassen) / Sigmoid (1 klasse) bij classificatie.<br />
<!--ID: 1610223328074--></p>
</div>

      <div class="backlinks">

<ul>
<li><a href="introductie-neurale-netwerken.html">Introductie Neurale Netwerken</a></li>
</ul>

</div>


    </article>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
   mermaid.initialize();
  </script>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>

  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>

</body>

</html>
